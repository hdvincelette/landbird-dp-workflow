---
title: "Landbird Data Processing Workflow"
name: Hannah
date: "`r format(Sys.time(), '%B %Y')`"
output:
  html_notebook:
    toc: yes
    number_sections: yes
  html_document:
    toc: yes
    df_print: paged
address: Alaska Regional Office 1011 E Tudor Rd, Anchorage, AK
email: hannah_vincelette@fws.gov
github: "https://github.com/hdvincelette/landbird-dp-workflow"
surname: Vincelette
position: Wildlife Biologist
editor_options:
  chunk_output_type: console
---

# Overview

The purpose of this document is to provide a reproducible workflow for processing data collected annually by the Landbird section of USFWS Alaska Migratory Bird Management. Data processing is just one component of data management. For complete guidance, view the [Alaska Region Interim Data Management User Guide.](https://ak-region-dst.gitbook.io/alaska-region-interim-data-management-user-guide/)

#### What is an R Notebook?

["An R Notebook is an R Markdown document with chunks that can be executed independently and interactively, with output visible immediately beneath the input." - R Markdown: The Definitive Guide](https://bookdown.org/yihui/rmarkdown/notebook.html)

#### How to use this guide

This document is meant to be interactive, with text prompts and "code chunks" which may require input before being executed. The following workflow was created for tabular data collected annually for projects archived on the Regional Data Repository. While effort is made to generalize code to all actively collected data, further edits may be required to process data in an standardized format. In this case, it is recommended users duplicate and rename the "landbird-dp-workflow.Rmd" before making changes.

# Workspace setup

### Install R packages

Run the following installation code.

```{r include=FALSE, results=TRUE}

# Install missing packages
if (!require("pacman"))
  install.packages("pacman")
pacman::p_load("devtools", "move", "auk", "dplyr")

# (optional) Set GitHub download file method
options(download.file.method = "wininet")

# Install FWSAkRDRtools from GitHub
devtools::install_github("hdvincelette/FWSAkRDRtools")

# Install mdJSONdictio from GitHub
devtools::install_github("hdvincelette/mdJSONdictio", ref="master")

# Uninstall packages (if necessary)
remove.packages("FWSAkRDRtools")
remove.packages("mdJSONdictio")
.rs.restartR()


```

If the installations fail, refer to [Install mdJSONdictio R package](https://hdvincelette.github.io/mdJSONdictio/articles/03_Setup_mdJSONdictio.html#install-mdjsondictio-r-package) for troubleshooting tips.

# Data preprocessing

### Download authoritative copies of project files

Open the [Migratory Bird Management Regional Data Repository](file://ifw7ro-file.fws.doi.net/datamgt/mbm/) and navigate to the project folder. If you are working from a remote location, you must be connected to the VPN (Virtual Private Network) to access this folder.

Paste the names of the project folder, data dictionary(ies), and data entry template(s).

```{r include=FALSE}

project.folder <- "mbmlb_003_Red_Knot_Breeding"
dictionary.list <- c("dictionary")
template.list <- c("template")
local.path <- "C:/Users/hvincelette/OneDrive - DOI/Documents/GitHub/landbird-dp-workflow/downloads"

```

Now run the following code chunk.

```{r include=FALSE, results=FALSE}

FWSAkRDRtools::download.files(
  pattern = c(dictionary.list, template.list),
  project = project.folder,
  path = local.path,
  main = TRUE,
  incoming = TRUE,
  recursive = TRUE
)

```

View the downloaded data dictionary(ies) and data entry template(s) in the "downloads" sub-folder of this R project. You can navigate here using the "Files" tab in the lower left pane. This should default to the project directory. If not, click "Session" in the toolbar, hover over "Set Working Directory," and click "To Project Directory." Click the "downloads" sub-folder, navigate to the blue cog wheel ("More file commands"), and select "Show Folder in New Window." A File Explorer window should open up to the "downloads" sub-folder. Verify all your data dictionaries and data entry templates are contained here.

*Optional: You can skip the above steps and download everything manually from the [Migratory Bird Management Regional Data Repository](file://ifw7ro-file.fws.doi.net/datamgt/mbm/) project folder. Data dictionaries and data entry templates should be available in the "metadata" sub-folder. Save a copy of data dictionary(ies) and data entry template(s) in an accessible location (i.e. the "downloads" sub-folder of this R project)*

### Enter scanned or hard copy data

Copy and save each data entry template in a preferred location for data entry (this can be in the "data-raw" sub-folder of this R project). Use a unique and descriptive name - refer to [Best Practices in Naming Conventions](https://ak-region-dst.gitbook.io/alaska-region-interim-data-management-user-guide/alaska-data-management-101/file-organization-and-best-practices/best-practices-in-naming-conventions). It's okay to save this file in an Excel file format to preserve validation rules.

Enter scanned or hard copy data according to the respective data dictionary(ies). Review entered data to check for errors. The Excel Filter function (Home\>Editing\>Filter) is especially useful for viewing unique values and value ranges.

Save the entered data file(s) in the "data-raw" sub-folder of this R project. Convert the file(s) to comma-separated values (CSV) format, if necessary. Make sure the correct sheet is selected when converting if more than one sheet is used (i.e., validation rules).

# Data quality checks

Before beginning, make sure all entered data and data dictionary(ies) are saved in the sub-folders of this R project ("data-raw" and "downloads", respectively).

### Summarize: capture, resight, nest, survey

SpeciesCode, Date, Location, Attachments (bands, tags), Morphometrics

```{r}
# Provide directory paths for the data file(s) and dictionary
raw.data.path<- "data-raw/"
dictionary.path<- "downloads/"

```

```{r}
# Import raw data file(s)
raw.data.files <-
  list.files(
    path = raw.data.path,
    pattern = NULL,
    full.names = FALSE,
    recursive = FALSE
  )

raw.data.choice <- utils::select.list(
  c(raw.data.files),
  multiple = TRUE,
  graphics = TRUE,
  title = cat(paste0("\nSelect one or more data files to import."))
)

raw.data.ext <- tools::file_ext(raw.data.choice)
import.data.name <-
  stringr::str_replace(raw.data.choice, paste0(".", raw.data.ext), "")


while (TRUE %in% duplicated(import.data.name)) {
  message(cat(
    "\n\n\nError: The following files have the same name.",
    paste0("\n  ", raw.data.choice[dup.test])
  ))
  
  raw.data.choice <- utils::select.list(
    c(raw.data.files),
    multiple = TRUE,
    graphics = TRUE,
    title = cat(paste0(
      "\n...Reselect one or more data files to import."
    ))
  )
  
  raw.data.ext <- tools::file_ext(raw.data.choice)
  import.data.name <-
    stringr::str_replace(raw.data.choice, paste0(".", raw.data.ext), "")
  
}

raw.data.list <- list()

for (a in 1:length(raw.data.ext)) {
  if (raw.data.ext[a] %in% c("xlsx", "xls")) {
    import.file <-
      readxl::read_excel(paste0(raw.data.path, raw.data.choice[a]))
    raw.data.list[[a]] <- import.file
    
  } else if (raw.data.ext[a] %in% c("csv")) {
    import.file <-
      utils::read.csv(paste0(raw.data.path, raw.data.choice[a]))
    raw.data.list[[a]] <- import.file
    
  }
}

names(raw.data.list) <- import.data.name


# Import data dictionary
dictionary.files <-
  list.files(
    path = dictionary.path,
    pattern = NULL,
    full.names = FALSE,
    recursive = FALSE
  )

dictionary.choice <- utils::select.list(
  c(dictionary.files),
  multiple = FALSE,
  graphics = TRUE,
  title = cat(paste0("\nSelect a dictionary to import."))
)


dictionary.ext <- tools::file_ext(dictionary.choice)

if (dictionary.ext %in% c("xlsx", "xls")) {
  ref.dictionary <-
    readxl::read_excel(paste0(dictionary.path, dictionary.choice))
  
} else if (dictionary.ext %in% c("csv")) {
  ref.dictionary <-
    utils::read.csv(paste0(dictionary.path, dictionary.choice))
  
} else if (dictionary.ext %in% c("json")) {
  ref.dictionary <-
    rjson::fromJSON(file = paste0(getwd(), "/", dictionary.path, dictionary.choice))
  
}

```

### Validate: data file(s) v. dictionary

```{r}
# Validate data file(s) against the dictionary
warning.list <- list()


for (a in 1:length(raw.data.list)) {
  
  assign("input.dxnry", ref.dictionary)
  
  assign("input.data", raw.data.list[[a]])
  
  if (dictionary.ext %in% c("json")) {
    warning.list[[a]] <-
      mdJSONdictio::validate.mdJSON(
        x = input.dxnry,
        y = input.data
      )
  } else {
    warning.list[[a]] <-
      mdJSONdictio::validate.table(x = input.dxnry,
                                   y = input.data)
  }
}

names(warning.list) <- import.data.name
                                                        

```

### Reconcile: correct data files

```{r}
# display unique values..readlines / replace

```

### Reconcile: correct data dictionary

```{r}

req.attribute.fields <-
      c("codeName", "allowNull", "dataType", "definition", "domainId")

for (a in 1:length(warning.list)) {
  warn.table <- warning.list[[a]]
  warn.table$Category[warn.table$Category == "CodeName"] <- "codeName"
  
  
  # codeName
  codeName.warn<- warn.table$Variable[warn.table$Category=="codeName"]
  
  codeName.choice <- utils::select.list(
    c(codeName.warn, "NONE"),
    multiple = TRUE,
    graphics = TRUE,
    title = cat(
      paste0(
        "\nSome variables in the data file '",
        import.data.name[a],
        "' were not found in the data dictionary.\nSelect which, if any, to add to the data dictionary.\n"
      )
    )
  )
  if (!"NONE" %in% codeName.choice) {
    new.dictionary<- ref.dictionary
    for (b in 1:length(codeName.choice)) {
      
      if(dictionary.ext %in% c("json")){
      new.dictionary <-
        modify.mdJSON(
          x = new.dictionary,
          how = "add_attribute",
          attribute_codeName = codeName.choice[b],
          attribute_allowNull = NULL,
          attribute_dataType = NULL,
          attribute_definition = NULL,
          attribute_units = NULL,
          attribute_unitsResolution = NULL,
          attribute_isCaseSensitive = NULL,
          attribute_missingValue = NULL,
          attribute_minValue = NULL,
          attribute_maxValue = NULL,
          attribute_fieldWidth = NULL,
          domainItem = NULL,
          quiet = FALSE)
      } else {
        new.dictionary <-
          mdJSONdictio::modify.table(
          x = ref.dictionary,
          how = "add_attribute",
          attribute_codeName = codeName.choice[b],
          attribute_allowNull = NULL,
          attribute_dataType = NULL,
          attribute_definition = NULL,
          attribute_units = NULL,
          attribute_unitsResolution = NULL,
          attribute_isCaseSensitive = NULL,
          attribute_missingValue = NULL,
          attribute_minValue = NULL,
          attribute_maxValue = NULL,
          attribute_fieldWidth = NULL,
          domainItem = NULL,
          quiet = FALSE
          )
      }
    }
  }
  
  # domainItem_value
  domainItem_value.table <-
    warn.table %>% dplyr::filter(Category == "domainItem_value")
  
  for(b in 1:nrow(domainItem_value.table)){
    domainItem.vector<- unlist(strsplit(gsub(
      ".*: ", "", domainItem_value.table$Message[b]
    ), ", ", perl = TRUE))
    
    domainItem.choice <-
      utils::select.list(
        c(domainItem.vector, "NONE"),
        multiple = TRUE,
        graphics = TRUE,
        title = cat(
          paste0(
            "\nThe attribute '", domainItem_value.table$Variable[b] ,"' in the data file '",
            import.data.name[a],
            "' contains entry values not found in the data dictionary.\nSelect which, if any, to add to the domain item list in the data dictionary.\n"
          )
        )
      )
    
    if (!"NONE" %in% domainItem.choice) {
      for (c in 1:length(domainItem.choice)) {
        new.dictionary <-
          modify.mdJSON(
            x = ref.dictionary,
            how = "add_domainItem",
            domain_codeName = domainItem_value.table$Variable[b],
            domainItem_value = domainItem.choice[c],
            domainItem_name = NULL,
            domainItem_definition = NULL,
            quiet = FALSE
          )
      }
      
    }
  }
  
  # dataType
  
  dataType.table <-
    warn.table %>% dplyr::filter(Category == "dataType_RDatatype")
  
  for (b in 1:nrow(dataType.table)) {
    
    detected.dataType <-
      gsub("[\\(\\)]",
           "",
           regmatches(
             dataType.table$Message[b],
             gregexpr("\\(.*?\\)", dataType.table$Message[b])
           )[[1]])[1]
    dxnry.dataType <-
      gsub("[\\(\\)]",
           "",
           regmatches(
             dataType.table$Message[b],
             gregexpr("\\(.*?\\)", dataType.table$Message[b])
           )[[1]])[2]
    
    dataType.choice <-
      utils::select.list(
        c("yes","no"),
        multiple = FALSE,
        graphics = FALSE,
        title = cat(
          paste0(
            "\nThe attribute '",
            dataType.table$Variable[b] ,
            "' in the data file '",
            import.data.name[a],
            "' has entry values with a different dataType (",detected.dataType,") than indicated in the dictionary (",dxnry.dataType,").\nWould you like to update the dataType in the dictionary?\n"
          )
        )
      )
    
    if ("yes" %in% dataType.choice) {
        new.dictionary <-
          modify.mdJSON(
            x = ref.dictionary,
            how = "update_attribute",
            attribute_codeName = dataType.table$Variable[b],
            attribute_allowNull = NULL,
            attribute_dataType = detected.dataType,
            attribute_definition = NULL,
            attribute_units = NULL,
            attribute_unitsResolution = NULL,
            attribute_isCaseSensitive = NULL,
            attribute_missingValue = NULL,
            attribute_minValue = NULL,
            attribute_maxValue = NULL,
            attribute_fieldWidth = NULL,
            domain_codeName = NULL,
            new.attribute_codeName = NULL,
            quiet = FALSE
          )
      
    }
  }

  check.dictionary <-
    rjson::fromJSON(new.dictionary[["data"]][[1]][["attributes"]][["json"]])
  
  
  new.json = rjson::toJSON(x = new.dictionary)
  write(x = new.json, file = "test/test.dictionary.json")

}

  
```

### Quality control checks: visualization

```{r}
# continuous: box plots
# discrete: range of values (interactive table)
# plot lat/long
# uniqe value check - band number, flag code, colorband combo
# BBL status
# Recaptures â€“ In columns have values
# BandNumberOut + BBLDisposition = 1 once
# Duplicate rows

```

```{r}
# Add uuids
for (a in 1:length(raw.data.list)) {
  data <-  raw.data.list[[a]]
  
  if ("OccurenceID" %in% colnames(data) == FALSE) {
    data <- cbind(OccurenceID = NA, data)
  }
  
  for (b in 1:nrow(data)) {
    if (is.na(data$OccurenceID[b]) == TRUE ||
        data$OccurenceID[b] == "") {
      data$OccurenceID[b] = uuid::UUIDgenerate(use.time = FALSE)
    }
  }
  
  raw.data.list[[a]] <- data
  
}

```

# Merge data file with authoritative copy

```{r}
#
```

# Data quality checks (cont.)

### Validation: data file v. dictionary

```{r}
#
```

# Data postprocessing

uuids, versioning

```{r}
#
```

# Archive in the Regional Data Repository

```{r}
project = "mbmlb_004_Eielson"
path = "C:/Users/hvincelette/OneDrive - DOI/Documents/Data_management/landbirds_project_maintenance/Archived_Projects/Eielson"

```

```{r}
#
commit <-
  FWSAkRDRtools::commit.files(project = project,
               local.folder = path,
               recursive = TRUE)


```

# Publish to Science Base

# Capture data transformations

### Biological samples inventory

```{r}
# update.bioinventory
```

### Bander portal

```{r}
# Provide directory paths for the data file(s)
raw.data.path<- "data-raw/"
```

```{r}
# Import raw data file(s)
raw.data.files <-
  list.files(
    path = raw.data.path,
    pattern = NULL,
    full.names = FALSE,
    recursive = FALSE
  )

raw.data.choice <- utils::select.list(
  c(raw.data.files),
  multiple = TRUE,
  graphics = TRUE,
  title = cat(paste0("\nSelect one or more data files to import."))
)

raw.data.ext <- tools::file_ext(raw.data.choice)
import.data.name <-
  stringr::str_replace(raw.data.choice, paste0(".", raw.data.ext), "")


while (TRUE %in% duplicated(import.data.name)) {
  message(cat(
    "\n\n\nError: The following files have the same name.",
    paste0("\n  ", raw.data.choice[dup.test])
  ))
  
  raw.data.choice <- utils::select.list(
    c(raw.data.files),
    multiple = TRUE,
    graphics = TRUE,
    title = cat(paste0(
      "\n...Reselect one or more data files to import."
    ))
  )
  
  raw.data.ext <- tools::file_ext(raw.data.choice)
  import.data.name <-
    stringr::str_replace(raw.data.choice, paste0(".", raw.data.ext), "")
  
}

raw.data.list <- list()

for (a in 1:length(raw.data.ext)) {
  if (raw.data.ext[a] %in% c("xlsx", "xls")) {
    import.file <-
      readxl::read_excel(paste0(raw.data.path, raw.data.choice[a]), na = "")
    raw.data.list[[a]] <- import.file
    
  } else if (raw.data.ext[a] %in% c("csv")) {
    import.file <-
      utils::read.csv(paste0(raw.data.path, raw.data.choice[a]), na.strings = "")
    raw.data.list[[a]] <- import.file
    
  }
}

names(raw.data.list) <- import.data.name


```

```{r}

cap_data <- plyr::ldply(raw.data.list)

# Remove NA columns
missing_values<- NA

cap_data <- cap_data %>%
  dplyr::mutate_if(is.character,
                   list(~ dplyr::na_if(., missing_values))) %>%
  dplyr::mutate_if(is.character,
                   list(~ dplyr::na_if(., "")),
                   is.character,
                   list(~ dplyr::na_if(., "NULL"))) %>%
  dplyr::select(where(~ !all(is.na(.x))))

# Recode

## Locations
loc.cols <- c("City", "Area")
loc.cols <- loc.cols[loc.cols %in% colnames(cap_data)]

cap_data <- cap_data %>%
  dplyr::mutate_at(
    loc.cols,
    ~ dplyr::recode(
      .,
      "Eielson" = "EIEL",
      "Delta Junction" = "DELTA",
      "Eareckson" = "EARECK",
      "Beluga" = "BELUGA",
      "King Salmon" = "KING",
      "Nome" = "NOME",
      "Shemya" = "EARECK",
      "Anchorage" = "ANC",
      "Chicken" = "CHICKEN",
      "Fort Yukon" = "FORTYUK"
    )
  ) %>%
  tidyr::unite(
    LocationID,
    all_of(loc.cols),
    sep = "",
    remove = TRUE,
    na.rm = TRUE
  )

## Capture method
cap_data <- cap_data %>%
  dplyr::mutate_at(
    "CaptureMethod",
    ~ dplyr::recode(
      .,
      "MN" = "Mist net",
      "WT" = "Walk-in trap",
      "HA" = "Hand capture",
      "BN" = "Bow net",
      "CN" = "Cannon net",
      "WN" = "Whoosh net",
      "NG" = "Net guns"
    )
  )

## Bander/Scribe ID
Master_Contacts <- read_csv("~/Data_management/landbirds_project_maintenance/Reformatted_data/Master_Contacts.csv")

cap_data <- cap_data %>%
  dplyr::mutate_at(
    c("BanderID", "ScribeID"),
    ~ plyr::mapvalues(
      .,
      from = Master_Contacts$FWSCode_FMLast,
      to = Master_Contacts$FWSCode_FML,
      warn_missing = FALSE
    )
  ) %>%
  dplyr::mutate_at(
    c("BanderID", "ScribeID"),
    ~ plyr::mapvalues(
      .,
      from = Master_Contacts$FWSCode_FLast,
      to = Master_Contacts$FWSCode_FL,
      warn_missing = FALSE
    )
  )

setdiff(
  c(cap_data$BanderID, cap_data$ScribeID),
  c(Master_Contacts$FWSCode_FL, Master_Contacts$FWSCode_FML)
)
                  

## Leg attachments
### For each leg attachment column, create new column with paste0("_color"), copy over partial match colors, replace values with codes, recode colors, recod markers

leg_cols <-   c(
  "ULIn",
  "LLIn",
  "URIn",
  "LRIn",
  "ULOut",
  "LLOut",
  "UROut",
  "LROut"
)

# na.cols<- colnames(cap_data)[sapply(cap_data, function(x)all(is.na(x)))]
# leg_cols <- leg_cols[leg_cols %in% colnames(cap_data) & !leg_cols %in% na.cols]

leg_cols <- leg_cols[leg_cols %in% colnames(cap_data)]


# test <- cap_data %>%
#   dplyr::filter_at(leg_cols, dplyr::any_vars(stringr::str_detect(., stringr::regex("-", ignore_case =
#                                                                                      TRUE))))

my_colors <-
  c("Bk",
    "Br",
    "Db",
    "Dg",
    "Lg",
    "Gy",
    "Hp",
    "Lb",
    "Mg",
    "Mv",
    "Or",
    "Pp",
    "Rd",
    "Wh",
    "Yl"
    )

BBL_colors <-
  c(
    "Black",
    "Brown",
    "Dark Blue",
    "Dark Green",
    "Light Green",
    "Gray",
    "Hot Pink",
    "Light Blue",
    "Magenta",
    "Mauve",
    "Orange",
    "Purple",
    "Red",
    "White",
    "Yellow"
  )

cap_data <-  cap_data %>%
  dplyr::mutate_if(is.logical, as.factor) %>%
  dplyr::rowwise() %>%
  dplyr::mutate_at(
    c(leg_cols, "FlagColorIn", "FlagColorOut"),
    ~ stringi::stri_replace_all_regex(.,
                                      my_colors,
                                      BBL_colors,
                                      vectorize_all = FALSE)
  ) %>%
  dplyr::mutate_at(leg_cols,
                   ~ stringi::stri_replace_all_regex(.,
                                                     "-",
                                                     "/",
                                                     vectorize_all = FALSE)) %>%
  dplyr::mutate(
    across(.cols = leg_cols,
           .names = "{paste0(color_cols[!grepl('Flag', color_cols)])}")
  ) %>%
  dplyr::mutate_at(
    leg_cols,
    ~ stringi::stri_replace_all_regex(.,
                                      BBL_colors,
                                      "01A",
                                      vectorize_all = FALSE)
  ) %>%
  dplyr::mutate_at(leg_cols,
                   ~ stringi::stri_replace_all_regex(.,
                                                     c("X", "Geo", "Flag"),
                                                     c("00", "90C", "69"),
                                                     vectorize_all = FALSE)) %>%
  dplyr::mutate_at(
    color_cols,
    ~ ifelse(
      grepl("/", .),
      stringi::stri_replace_all_regex(.,
                                      c("X", "Geo", "Flag"),
                                      "", vectorize_all = FALSE),
      stringi::stri_replace_all_regex(.,
                                      c("X", "Geo", "Flag"),
                                      NA, vectorize_all = FALSE)
    )
  ) %>%
  dplyr::mutate_at(color_cols,
                   ~ ifelse(
                     grepl("/$", .),
                     stringi::stri_replace_all_regex(.,
                                                     "/",
                                                     "", vectorize_all = FALSE),
                     .
                   ))

 
 # QC checks
bander_portal_species_measurements <-
  bander_portal_species_measurements %>%
  dplyr::mutate_if(is.numeric, ~ dplyr::na_if(., 0)) %>%
  dplyr::rowwise() %>%
  dplyr::mutate_at("bandSize", ~ ifelse(grepl(", ", .), as.list(strsplit(., ", ")), as.list(.)))
    

bandsize_col<- "BandSize"
mass_col<- "Mass_g"
wingchord_col<- "Wing_mm"
taillength_col<- "Tail_mm"
sex_col<- "FieldSex"
female_code<- "F"
male_code<- "M"
unknown_code<- "U"

 
for (a in c("HHHH", unique(cap_data$SpeciesCode))) {
  ## Species measurements
  species.num <-
    which(bander_portal_species_measurements$alphaCode == a)
  if (length(species.num) == 0) {
    warning(paste0("SpeciesCode '", a, "' is invalid."))
    next()
  } else {
    sp_mydata <- cap_data %>%
      dplyr::filter(SpeciesCode == a)
    
    sp_BBLdata <- bander_portal_species_measurements %>%
      dplyr::filter(alphaCode == a)
    
    ### Band size
    if (length(setdiff(unique(na.omit(sp_mydata[[bandsize_col]])), unlist(na.omit(
      sp_BBLdata$bandSize
    ))))
    != 0) {
      warning(
        paste0(
          "One or more ",
          sp_BBLdata$commonName,
          " has a band size inconsistant with BBL recommendations."
        )
      )
    }
    
    ### Maximum mass
    if (is.na(sp_BBLdata$femaleMass_max_g) == FALSE &
        is.na(sp_BBLdata$maleMass_max_g) == FALSE) {
      if (max(na.omit(sp_mydata[[mass_col]][sp_mydata[[sex_col]] == female_code])) > sp_BBLdata$femaleMass_max_g) {
        warning(
          paste0(
            "One or more female ",
            sp_BBLdata$commonName,
            " have a higher mass than expected for the species."
          )
        )
        
      }
      
      if (max(na.omit(sp_mydata[[mass_col]][sp_mydata[[sex_col]] == male_code])) > sp_BBLdata$maleMass_max_g) {
        warning(
          paste0(
            "One or more male ",
            sp_BBLdata$commonName,
            " have a higher mass than expected for the species."
          )
        )
        
      }
      
      if (max(na.omit(sp_mydata[[mass_col]][sp_mydata[[sex_col]] %in% c(NA, unknown_code)])) >
          max(sp_BBLdata$femaleMass_max_g,
              sp_BBLdata$maleMass_max_g)) {
        warning(
          paste0(
            "One or more ",
            sp_BBLdata$commonName,
            " (unknown sex) have a higher mass than expected for the species."
          )
        )
        
      }
    }
    
    ### Minimum mass
    if (is.na(sp_BBLdata$femaleMass_min_g) == FALSE &
        is.na(sp_BBLdata$maleMass_min_g) == FALSE) {
      if (min(na.omit(sp_mydata[[mass_col]][sp_mydata[[sex_col]] == female_code])) < sp_BBLdata$femaleMass_min_g) {
        warning(
          paste0(
            "One or more female ",
            sp_BBLdata$commonName,
            " have a lower mass than expected for the species."
          )
        )
        
      }
      
      if (min(na.omit(sp_mydata[[mass_col]][sp_mydata[[sex_col]] == male_code])) < sp_BBLdata$maleMass_min_g) {
        warning(
          paste0(
            "One or more male ",
            sp_BBLdata$commonName,
            " have a lower mass than expected for the species."
          )
        )
        
      }
      
      if (min(na.omit(sp_mydata[[mass_col]][sp_mydata[[sex_col]] %in% c(NA, unknown_code)])) <
          min(sp_BBLdata$femaleMass_min_g,
              sp_BBLdata$maleMass_min_g)) {
        warning(
          paste0(
            "One or more ",
            sp_BBLdata$commonName,
            " (unknown sex) have a lower mass than expected for the species."
          )
        )
        
      }
    }
    
    ### Maximum wing chord
    if (is.na(sp_BBLdata$femaleWingChord_max_mm) == FALSE &
        is.na(sp_BBLdata$maleWingChord_max_mm) == FALSE) {
      if (max(na.omit(sp_mydata[[wingchord_col]][sp_mydata[[sex_col]] == female_code])) > sp_BBLdata$femaleWingChord_max_mm) {
        warning(
          paste0(
            "One or more female ",
            sp_BBLdata$commonName,
            " have a longer wing chord than expected for the species."
          )
        )
        
      }
      
      if (max(na.omit(sp_mydata[[wingchord_col]][sp_mydata[[sex_col]] == male_code])) > sp_BBLdata$maleWingChord_max_mm) {
        warning(
          paste0(
            "One or more male ",
            sp_BBLdata$commonName,
            " have a longer wing chord than expected for the species."
          )
        )
        
      }
      
      if (max(na.omit(sp_mydata[[wingchord_col]][sp_mydata[[sex_col]] %in% c(NA, unknown_code)])) >
          max(sp_BBLdata$femaleWingChord_max_mm,
              sp_BBLdata$maleWingChord_max_mm)) {
        warning(
          paste0(
            "One or more ",
            sp_BBLdata$commonName,
            " (unknown sex) have a longer wing chord than expected for the species."
          )
        )
        
      }
    }
    
    ### Minimum wing chord
    if (is.na(sp_BBLdata$femaleWingChord_min_mm) == FALSE &
        is.na(sp_BBLdata$maleWingChord_min_mm) == FALSE) {
      if (min(na.omit(sp_mydata[[wingchord_col]][sp_mydata[[sex_col]] == female_code])) < sp_BBLdata$femaleWingChord_min_mm) {
        warning(
          paste0(
            "One or more female ",
            sp_BBLdata$commonName,
            " have a shorter wing chord than expected for the species."
          )
        )
        
      }
      
      if (min(na.omit(sp_mydata[[wingchord_col]][sp_mydata[[sex_col]] == male_code])) < sp_BBLdata$maleWingChord_min_mm) {
        warning(
          paste0(
            "One or more male ",
            sp_BBLdata$commonName,
            " have a shorter wing chord than expected for the species."
          )
        )
        
      }
      
      if (min(na.omit(sp_mydata[[wingchord_col]][sp_mydata[[sex_col]] %in% c(NA, unknown_code)])) <
          min(sp_BBLdata$femaleWingChord_min_mm,
              sp_BBLdata$maleWingChord_min_mm)) {
        warning(
          paste0(
            "One or more ",
            sp_BBLdata$commonName,
            " (unknown sex) have a shorter wing chord than expected for the species."
          )
        )
        
      }
    }
    
    ### Maximum tail length
    if (is.na(sp_BBLdata$femaleTailLength_max_mm) == FALSE &
        is.na(sp_BBLdata$maleTailLength_max_mm) == FALSE) {
      if (max(na.omit(sp_mydata[[taillength_col]][sp_mydata[[sex_col]] == female_code])) > sp_BBLdata$femaleTailLength_max_mm) {
        warning(
          paste0(
            "One or more female ",
            sp_BBLdata$commonName,
            " have a longer tail than expected for the species."
          )
        )
        
      }
      
      if (max(na.omit(sp_mydata[[taillength_col]][sp_mydata[[sex_col]] == male_code])) > sp_BBLdata$maleTailLength_max_mm) {
        warning(
          paste0(
            "One or more male ",
            sp_BBLdata$commonName,
            " have a longer tail than expected for the species."
          )
        )
        
      }
      
      if (max(na.omit(sp_mydata[[taillength_col]][sp_mydata[[sex_col]] %in% c(NA, unknown_code)])) >
          max(sp_BBLdata$femaleTailLength_max_mm,
              sp_BBLdata$maleTailLength_max_mm)) {
        warning(
          paste0(
            "One or more ",
            sp_BBLdata$commonName,
            " (unknown sex) have a longer tail than expected for the species."
          )
        )
        
      }
    }
    
    ### Minimum tail length
    if (is.na(sp_BBLdata$femaleTailLength_min_mm) == FALSE &
        is.na(sp_BBLdata$maleTailLength_min_mm) == FALSE) {
      if (min(na.omit(sp_mydata[[taillength_col]][sp_mydata[[sex_col]] == female_code])) < sp_BBLdata$femaleTailLength_min_mm) {
        warning(
          paste0(
            "One or more female ",
            sp_BBLdata$commonName,
            " have a shorter tail than expected for the species."
          )
        )
        
      }
      
      if (min(na.omit(sp_mydata[[taillength_col]][sp_mydata[[sex_col]] == male_code])) < sp_BBLdata$maleTailLength_min_mm) {
        warning(
          paste0(
            "One or more male ",
            sp_BBLdata$commonName,
            " have a shorter tail than expected for the species."
          )
        )
        
      }
      
      if (min(na.omit(sp_mydata[[taillength_col]][sp_mydata[[sex_col]] %in% c(NA, unknown_code)])) <
          min(sp_BBLdata$femaleTailLength_min_mm,
              sp_BBLdata$maleTailLength_min_mm)) {
        warning(
          paste0(
            "One or more ",
            sp_BBLdata$commonName,
            " (unknown sex) have a shorter tail than expected for the species."
          )
        )
        
      }
    }
    
    
    
    
    
  }
}
   
   

## lookup tables


# Add missing columns
cap_data[setdiff(na.omit(band_aux_cols$my_col), colnames(cap_data))] <- as.character(NA)
 
 
 colors <- bander_portal_lookups %>%
  dplyr::filter(field == "Color") %>%
  dplyr::pull(description)
 
 
 ## Partition by disposition
 
cap_data <- split(cap_data, cap_data$BBLDisposition)

cap_data<- lapply(cap_data, function(x) {
  x %>%
    dplyr::mutate_if(is.character, ~ dplyr::na_if(., ''))
})
 



```

# Survey data transformations

### ebird

```{r}
# survey.to.ebird
library("auk")

# ebird
# usfwslandbirds
# fwslandbirds
# API key: e2m7tt6sfo4n


```

# Tracking data

### Movebank import

Verify the stored login credentials are correct, and add/remove credentials as needed.

```{r warning=FALSE, include=FALSE, results=FALSE}

# Check existing login credentials
keyring::key_list()

# Add login credentials
move2::movebank_store_credentials(username = "mbmlandbirds",
                                  password = "AKmove2018!",
                                  key_name = "myOtherAccount")

# Remove login credentials
move2::movebank_remove_credentials(key_name = getOption("move2_movebank_key_name"))


```

*Optional: If you want to import Movebank data from a secondary account, paste the associated service name listed in the key list (e.g., "myOtherAccount") and run the following code. Note the account with the service name "movebank" is the default, so you only need to run this code if you want to access a different account. Restart R or execute* `options("move2_movebank_key_name" = "movebank")` *to* *return to the default account. See* `vignette("movebank", package="move2")` *for more details.*

```{r}

keyring::key_list()

options("move2_movebank_key_name" = "myOtherAccount")

```

Run the following code to import location data from one or more studies into R. Single study imports have the option of selecting a time range of location data, whereas multiple study imports include all location data. By default, the location and reference data are both imported

```{r warning=FALSE, include=FALSE, results=FALSE}
          
# Import Movebank location and/or reference data
import <- get.move.data(
  location = TRUE,
  reference = TRUE,
  remove_movebank_outliers <- FALSE,
  omit_derived_data <- FALSE
)

save(import, import.2, file = "man/import.RData")

# Unlist dataframes (if necessary)
if(any(sapply(import, function(x)
  any(class(x) == "list")))==TRUE) {
  import.dfs <- rrapply::rrapply(
    import,
    classes = "data.frame",
    how = "flatten",
    options = list(namesep = "-",
                   simplify = FALSE)
  )
} else {
  import.dfs<- import
}

# Update dataField names
import.dfs <-
  lapply(import.dfs, function(x)
    setNames(x,
             sub(
               pattern = "_", replacement = "-", names(x)
             )))

import.dfs <-
  lapply(import.dfs, function(x)
    setNames(x,
             sub(
               pattern = "argos-", replacement = "argos:", names(x)
             )))


#!# only grabs the first items colnames
df<- tibble::as_tibble.list(import.dfs[[1]])
loc.cols<- colnames(dplyr::select(df, matches("location")))


names(import.dfs[[1]][["argos:location_1"]][[1]][1])
import.dfs[[1]][["argos:location_1"]][[1]][[1]]


renquote <- function(l) if (is.list(l)) lapply(l, renquote) else enquote(l)
test<- lapply(unlist(renquote(import.dfs[1])), eval)
test2<- lapply(unlist(renquote(test)), eval)



for(a in 1:length(loc.cols)) {
  for (b in 1:length(df)) {
    if (sf::st_is_empty(df[b, loc.cols[a]]) == FALSE) {
      unnested.loc<- as.data.frame(sf::as_Spatial(df[b, loc.cols[a]]))
      colname.loc<- colnames(unnested.loc)
      unnested.long<- unnested.loc[1]
      unnested.lat<- unnested.loc[2]
      if(!colnames(unnested.long) %in% colnames(df)){
        df %>%
          tibble::add_column(colname.loc[1] = NA,
                             .before = paste0(loc.cols[a]))
      }
      if(!colnames(unnested.lat) %in% colnames(df)){
        
      }
      df$colnames(unnested.long)[b] <- unnested.long
      df$colnames(unnested.lat)[b] <- unnested.lat
    }
  }
}




# Save dataframes as separate objects 
for (i in 1:length(import.2)){
   assign(names(import.2[i]),as.data.frame(import.2[[i]]))
}
  

# Get updated movebank vocabulary
move.vocab <-
  move2::movebank_get_vocabulary(return_type = "list", omit_deprecated = FALSE)

move.vocab <- rrapply::rrapply(
  move.vocab,
  how = "bind",
  options = list(
    simplify = FALSE,
    coldepth = 3,
    namecols = TRUE
  )
) %>%
  tidyr::pivot_wider(
    names_from = "L2",
    values_from = "1",
    values_fill = NA,
    values_fn = unique
  ) %>%
  tidyr::unnest(colnames(.)) %>%
  dplyr::rename("dataField" = "L1") %>%
  dplyr::mutate(dataField = stringr::str_replace_all(dataField, c("argos " = "argos:", " " = "-")),
                altLabel = stringr::str_replace_all(altLabel, c("argos " = "argos:", " " = "-")))
  

colnames(test)[!colnames(test) %in% move.vocab$dataField]



```

### Transformations
